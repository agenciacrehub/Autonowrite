#!/usr/bin/env python3
"""
AutonoWrite - Sistema Multiagente para Gera√ß√£o de Conte√∫do
Vers√£o integrada para TCC com recursos gratuitos
"""

import os
import json
import time
from typing import Dict, List, Optional
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("‚ö†Ô∏è  Groq n√£o instalado. Use: pip install groq")

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

try:
    from dotenv import load_dotenv
    load_dotenv()
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False


class LLMProvider:
    """Provider base para diferentes modelos LLM"""
    
    def __init__(self, provider_type: str = "simulation"):
        self.call_count = 0
        self.provider_type = provider_type
        self.client = None
        
        if provider_type == "auto":
            self._setup_auto()
        elif provider_type == "groq":
            self._setup_groq()
        elif provider_type == "ollama":
            self._setup_ollama()
        elif provider_type == "simulation":
            self._setup_simulation()
    
    def _setup_auto(self):
        """Setup autom√°tico - prioriza modo de simula√ß√£o para testes"""
        print("‚ö†Ô∏è  Usando modo de simula√ß√£o por padr√£o")
        self._setup_simulation()
    
    def _setup_groq(self):
        """Configura Groq API"""
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            raise ValueError("GROQ_API_KEY n√£o encontrada. Configure no .env")
        
        self.client = Groq(api_key=api_key)
        self.provider_type = "groq"
        self.model_name = "mixtral-8x7b-32768"
        print("üöÄ Usando Groq API (mixtral-8x7b-32768)")
    
    def _setup_ollama(self):
        """Configura Ollama local"""
        self.provider_type = "ollama"
        self.model_name = "gemma:2b"  # Modelo mais leve para sistemas com menos mem√≥ria
        print(f"üè† Usando Ollama local ({self.model_name})")
        print("‚ÑπÔ∏è  Baixando o modelo pela primeira vez, pode demorar alguns minutos...")
        try:
            import ollama
            ollama.pull(self.model_name)  # Garante que o modelo est√° baixado
        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao baixar o modelo: {e}")
            raise
    
    def _setup_simulation(self):
        """Configura simula√ß√£o para desenvolvimento"""
        self.provider_type = "simulation"
        self.model_name = "simulado"
        print("üé≠ Usando simula√ß√£o (para desenvolvimento)")
        print("‚ÑπÔ∏è  Modo de simula√ß√£o ativado - usando respostas pr√©-definidas")
    
    def generate(self, prompt: str, max_tokens: int = 1500) -> str:
        """Gera resposta usando o provider configurado"""
        self.call_count += 1
        
        if self.provider_type == "groq":
            return self._generate_groq(prompt, max_tokens)
        elif self.provider_type == "ollama":
            return self._generate_ollama(prompt, max_tokens)
        else:
            return self._generate_simulation(prompt)
    
    def _generate_groq(self, prompt: str, max_tokens: int) -> str:
        """Gera√ß√£o via Groq"""
        try:
            completion = self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model_name,
                max_tokens=max_tokens,
                temperature=0.7
            )
            return completion.choices[0].message.content
        except Exception as e:
            return f"Erro Groq: {str(e)}"
    
    def _generate_ollama(self, prompt: str, max_tokens: int) -> str:
        """Gera√ß√£o via Ollama"""
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[{'role': 'user', 'content': prompt}]
            )
            return response['message']['content']
        except Exception as e:
            return f"Erro Ollama: {str(e)}"
    
    def _simulate_llm_response(self, prompt: str, max_tokens: int) -> str:
        """Simula uma resposta do LLM para desenvolvimento"""
        # Simula diferentes tipos de respostas baseado no conte√∫do do prompt
        if "planejamento" in prompt.lower() or "plano" in prompt.lower():
            return """
            Plano para o t√≥pico:
            1. Introdu√ß√£o ao conceito
            2. Explica√ß√£o detalhada
            3. Exemplos pr√°ticos
            4. Conclus√£o
            """
        elif "pesquisa" in prompt.lower():
            return """
            Pesquisa sobre o t√≥pico:
            - Fonte 1: Artigo cient√≠fico relevante
            - Fonte 2: Dados estat√≠sticos atualizados
            - Fonte 3: Estudos de caso
            """
        elif "avalia√ß√£o" in prompt.lower() or "cr√≠tica" in prompt.lower():
            return """
            Avalia√ß√£o cr√≠tica:
            - Pontos fortes: Clareza, organiza√ß√£o
            - Pontos fracos: Poderia ter mais exemplos
            - Pontua√ß√£o: 8.5/10
            """
        elif "cr√≠tico" in prompt.lower() or "editor" in prompt.lower():
            return self._simulate_critic()
        else:
            return f"Simula√ß√£o - Resposta para: {prompt[:100]}..."
    
    def _simulate_planner(self) -> str:
        return """
## Plano Estruturado

### 1. Introdu√ß√£o
- Contextualiza√ß√£o do tema
- Relev√¢ncia e import√¢ncia atual
- Objetivos do artigo

### 2. Fundamenta√ß√£o Te√≥rica  
- Conceitos fundamentais
- Estado da arte
- Revis√£o da literatura

### 3. An√°lise Cr√≠tica
- Vantagens e limita√ß√µes
- Casos de uso pr√°ticos
- Compara√ß√µes com alternativas

### 4. Discuss√£o e Implica√ß√µes
- Impactos na √°rea
- Desafios identificados
- Oportunidades futuras

### 5. Conclus√£o
- S√≠ntese dos pontos principais
- Contribui√ß√µes do trabalho
- Sugest√µes para pesquisas futuras

**Palavras-chave**: sistema multiagente, intelig√™ncia artificial, automa√ß√£o
"""
    
    def _simulate_researcher(self) -> str:
        return """
## Relat√≥rio de Pesquisa

### Fontes Identificadas:
1. "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations" (2008)
2. "Large Language Models: A Survey" - ArXiv 2024  
3. "CrewAI Framework Documentation" - Documenta√ß√£o oficial 2024
4. "Prompt Engineering for Large Language Models" - ACM 2024

### Dados Relevantes:
- Sistemas multiagente mostram 40-60% de melhoria em tarefas complexas
- LLMs apresentam taxa de alucina√ß√£o de 15-25% em tarefas factuais
- Ciclos de cr√≠tica reduzem alucina√ß√µes em at√© 70%
- CrewAI framework adotado por 500+ empresas em 2024

### Informa√ß√µes T√©cnicas:
- Arquitetura baseada em agentes especializados
- Uso de engenharia de prompt para orquestra√ß√£o
- Implementa√ß√£o de ciclos de feedback e refinamento
- M√©tricas de avalia√ß√£o: ROUGE, BLEU, avalia√ß√£o humana
"""
    
    def _simulate_writer(self) -> str:
        return """
# Sistemas Multiagente para Gera√ß√£o Automatizada de Conte√∫do

## Introdu√ß√£o

A evolu√ß√£o da intelig√™ncia artificial tem proporcionado avan√ßos significativos na automa√ß√£o de tarefas cognitivas complexas. Entre essas inova√ß√µes, os sistemas multiagente emergem como uma abordagem promissora para superar limita√ß√µes dos modelos monol√≠ticos tradicionais, especialmente no contexto de gera√ß√£o de conte√∫do.

## Fundamenta√ß√£o Te√≥rica

Os sistemas multiagente (MAS) representam uma arquitetura distribu√≠da onde m√∫ltiplos agentes aut√¥nomos colaboram para resolver problemas complexos. No contexto de gera√ß√£o de texto, essa abordagem permite a especializa√ß√£o de diferentes componentes: planejamento, pesquisa, reda√ß√£o e revis√£o.

A filosofia "dividir para conquistar" permite decompor a tarefa de escrita em subtarefas menores e mais gerenci√°veis, cada uma executada por um agente especializado com objetivos e compet√™ncias espec√≠ficas.

## An√°lise Cr√≠tica

A principal vantagem dos sistemas multiagente reside na capacidade de emular processos humanos de produ√ß√£o editorial. Um escritor profissional naturalmente alterna entre fases de pesquisa, estrutura√ß√£o, reda√ß√£o e revis√£o. O AutonoWrite replica esse processo atrav√©s de agentes especializados.

### Vantagens Identificadas:
- Redu√ß√£o significativa de alucina√ß√µes atrav√©s do ciclo cr√≠tico
- Melhoria iterativa da qualidade do conte√∫do
- Especializa√ß√£o permite maior profundidade t√©cnica
- Flexibilidade para diferentes tipos de conte√∫do

### Limita√ß√µes Observadas:
- Maior custo computacional devido a m√∫ltiplas itera√ß√µes
- Depend√™ncia da qualidade dos prompts de cada agente
- Complexidade de orquestra√ß√£o entre componentes

## Conclus√£o

O sistema AutonoWrite demonstra o potencial dos sistemas multiagente para superar limita√ß√µes dos modelos monol√≠ticos. Atrav√©s da especializa√ß√£o e colabora√ß√£o entre agentes, √© poss√≠vel produzir conte√∫do de maior qualidade, factualidade e coer√™ncia.

Os resultados preliminares indicam que a abordagem multiagente representa um avan√ßo significativo na automa√ß√£o de tarefas de conhecimento, com aplica√ß√µes promissoras em contextos acad√™micos, jornal√≠sticos e corporativos.
"""
    
    def _simulate_critic(self) -> str:
        return """
## Avalia√ß√£o Cr√≠tica do Conte√∫do

### Pontua√ß√£o Geral: 7.8/10

### Crit√©rios Detalhados:

**Estrutura e Organiza√ß√£o: 8.5/10**
‚úÖ Estrutura l√≥gica clara com introdu√ß√£o, desenvolvimento e conclus√£o
‚úÖ Transi√ß√µes adequadas entre se√ß√µes  
‚ö†Ô∏è Algumas se√ß√µes poderiam ser mais equilibradas

**Profundidade T√©cnica: 7.0/10**
‚úÖ Conceitos fundamentais bem explicados
‚úÖ Terminologia t√©cnica apropriada
‚ö†Ô∏è Poderia incluir mais exemplos pr√°ticos na se√ß√£o de an√°lise

**Clareza e Legibilidade: 8.5/10**
‚úÖ Linguagem clara e acess√≠vel
‚úÖ Fluxo narrativo coerente
‚úÖ Par√°grafos bem constru√≠dos

**Verificabilidade das Fontes: 6.5/10** 
‚ö†Ô∏è Menciona conceitos sem cita√ß√µes espec√≠ficas
‚ö†Ô∏è Faltam refer√™ncias diretas a estudos mencionados
‚ö†Ô∏è Necess√°rio incluir mais dados quantitativos

**Coer√™ncia Argumentativa: 8.0/10**
‚úÖ Argumentos consistentes ao longo do texto
‚úÖ Conclus√µes coerentes com o desenvolvimento
‚ö†Ô∏è Poderia fortalecer contrapontos

### Recomenda√ß√µes Espec√≠ficas:
1. Adicionar cita√ß√µes diretas para os dados quantitativos mencionados
2. Incluir exemplo pr√°tico de implementa√ß√£o na se√ß√£o 3
3. Expandir discuss√£o sobre limita√ß√µes √©ticas
4. Verificar se todos os termos t√©cnicos est√£o adequadamente definidos

**Status: APROVADO COM REVIS√ïES MENORES**"""
        
    def _generate_simulation(self, prompt: str) -> str:
        """Gera uma resposta simulada baseada no papel do agente"""
        # Simula diferentes tipos de respostas baseado no papel do agente
        if "planejador" in prompt.lower() or "plano" in prompt.lower():
            return self._simulate_planner()
        elif "pesquisador" in prompt.lower() or "pesquisa" in prompt.lower():
            return self._simulate_researcher()
        elif "redator" in prompt.lower() or "escrever" in prompt.lower():
            return self._simulate_writer()
        elif "cr√≠tico" in prompt.lower() or "avaliar" in prompt.lower():
            return self._simulate_critic()
        else:
            # Resposta gen√©rica para outros casos
            return self._simulate_llm_response(prompt, max_tokens=1000)


class AgentRole(Enum):
    PLANNER = "planejador"
    RESEARCHER = "pesquisador"
    WRITER = "redator"
    CRITIC = "cr√≠tico"


@dataclass
class TaskResult:
    agent_role: AgentRole
    content: str
    metadata: Dict
    timestamp: datetime
    success: bool = True


class Agent:
    """Agente especializado do sistema"""
    
    def __init__(self, role: AgentRole, llm_provider: LLMProvider):
        self.role = role
        self.llm = llm_provider
        self.memory = []
    
    def execute_task(self, task: str, context: Optional[str] = None) -> TaskResult:
        """Executa tarefa espec√≠fica do agente"""
        
        prompt = self._build_prompt(task, context)
        response = self.llm.generate(prompt)
        
        result = TaskResult(
            agent_role=self.role,
            content=response,
            metadata={
                "prompt_length": len(prompt),
                "response_length": len(response),
                "context_provided": context is not None
            },
            timestamp=datetime.now()
        )
        
        self.memory.append(result)
        return result
    
    def _build_prompt(self, task: str, context: Optional[str] = None) -> str:
        """Constr√≥i prompt espec√≠fico para cada agente"""
        
        prompts = {
            AgentRole.PLANNER: f"""
Voc√™ √© um PLANEJADOR ESTRAT√âGICO especialista em estrutura√ß√£o de conte√∫do acad√™mico e t√©cnico.

TAREFA: {task}

Crie um plano detalhado e bem estruturado para abordar este t√≥pico. Inclua:
- Se√ß√µes principais e subse√ß√µes
- Pontos-chave a serem abordados
- Sequ√™ncia l√≥gica de desenvolvimento
- Palavras-chave relevantes

Formate como um outline claro e hier√°rquico.
""",
            
            AgentRole.RESEARCHER: f"""
Voc√™ √© um PESQUISADOR ACAD√äMICO meticuloso e experiente.

TAREFA: {task}
PLANO DE REFER√äNCIA: {context or 'N√£o fornecido'}

Conduza uma pesquisa abrangente sobre o t√≥pico. Forne√ßa:
- Fontes confi√°veis e atuais (artigos, estudos, documenta√ß√£o)
- Dados quantitativos relevantes
- Informa√ß√µes t√©cnicas precisas
- Cita√ß√µes e refer√™ncias quando apropriado

Organize as informa√ß√µes de forma clara e estruturada.
""",
            
            AgentRole.WRITER: f"""
Voc√™ √© um REDATOR PROFISSIONAL com expertise em conte√∫do t√©cnico e acad√™mico.

TAREFA: {task}
CONTEXTO DISPON√çVEL: {context or 'N√£o fornecido'}

Escreva um texto completo, bem estruturado e envolvente. Garanta:
- Linguagem clara e apropriada para o p√∫blico-alvo
- Estrutura l√≥gica seguindo o plano fornecido
- Uso adequado das informa√ß√µes de pesquisa
- Transi√ß√µes suaves entre se√ß√µes
- Conclus√£o que sintetiza os pontos principais

Produza um texto coerente e de alta qualidade.
""",
            
            AgentRole.CRITIC: f"""
Voc√™ √© um CR√çTICO RIGOROSO e avaliador de qualidade editorial.

TEXTO PARA AVALIA√á√ÉO: {task}

Avalie o texto com base nos seguintes crit√©rios:
1. **Estrutura e Organiza√ß√£o** (0-10)
2. **Profundidade T√©cnica** (0-10)  
3. **Clareza e Legibilidade** (0-10)
4. **Verificabilidade das Fontes** (0-10)
5. **Coer√™ncia Argumentativa** (0-10)

Para cada crit√©rio:
- Forne√ßa uma pontua√ß√£o
- Identifique pontos fortes
- Aponte √°reas para melhoria
- Ofere√ßa sugest√µes espec√≠ficas

Conclua com:
- **Pontua√ß√£o Geral** (m√©dia ponderada)
- **Status**: APROVADO, APROVADO COM REVIS√ïES, ou REQUER REVIS√ÉO MAIOR
- **Recomenda√ß√µes priorit√°rias** para melhoria

Seja construtivo mas rigoroso na avalia√ß√£o.
"""
        }
        
        return prompts[self.role]


class AutonoWriteSystem:
    """Sistema principal AutonoWrite"""
    
    def __init__(self, llm_provider: LLMProvider):
        self.llm = llm_provider
        self.agents = {
            AgentRole.PLANNER: Agent(AgentRole.PLANNER, llm_provider),
            AgentRole.RESEARCHER: Agent(AgentRole.RESEARCHER, llm_provider),
            AgentRole.WRITER: Agent(AgentRole.WRITER, llm_provider),
            AgentRole.CRITIC: Agent(AgentRole.CRITIC, llm_provider)
        }
        self.max_iterations = 3
        self.min_quality_score = 8.0
        self.execution_log = []
    
    def generate_content(self, topic: str, max_iterations: Optional[int] = None) -> Dict:
        """Pipeline principal de gera√ß√£o"""
        
        if max_iterations:
            self.max_iterations = max_iterations
        
        print(f"\nüöÄ AutonoWrite iniciando para: {topic}")
        print(f"üìä Configura√ß√£o: {self.max_iterations} itera√ß√µes m√°ximas")
        print(f"ü§ñ Provider: {self.llm.provider_type} ({self.llm.model_name})")
        
        start_time = datetime.now()
        
        # Fase 1: Planejamento
        print(f"\nüìã Fase 1: Planejamento...")
        plan_result = self.agents[AgentRole.PLANNER].execute_task(
            f"Criar plano estruturado para artigo sobre: {topic}"
        )
        
        # Fase 2: Pesquisa  
        print(f"\nüîç Fase 2: Pesquisa...")
        research_result = self.agents[AgentRole.RESEARCHER].execute_task(
            f"Pesquisar informa√ß√µes detalhadas sobre: {topic}",
            context=plan_result.content
        )
        
        # Fase 3: Reda√ß√£o com ciclo cr√≠tico
        print(f"\n‚úçÔ∏è Fase 3: Reda√ß√£o e Refinamento...")
        
        current_draft = None
        iteration = 1
        final_approved = False
        critic_history = []
        
        while iteration <= self.max_iterations and not final_approved:
            print(f"\n   üìù Itera√ß√£o {iteration}/{self.max_iterations}")
            
            # Reda√ß√£o/revis√£o
            if current_draft is None:
                # Primeira reda√ß√£o
                context = f"PLANO:\n{plan_result.content}\n\nPESQUISA:\n{research_result.content}"
                task = f"Escrever artigo completo sobre: {topic}"
            else:
                # Revis√£o baseada em feedback
                context = f"FEEDBACK ANTERIOR:\n{critic_history[-1]['content']}"
                task = f"Revisar e melhorar este texto:\n\n{current_draft}"
            
            write_result = self.agents[AgentRole.WRITER].execute_task(task, context)
            current_draft = write_result.content
            
            # Avalia√ß√£o cr√≠tica
            print(f"   üîç Avalia√ß√£o cr√≠tica...")
            critic_result = self.agents[AgentRole.CRITIC].execute_task(current_draft)
            
            # Extrai score da avalia√ß√£o
            score = self._extract_score(critic_result.content)
            
            critic_history.append({
                'iteration': iteration,
                'content': critic_result.content,
                'score': score
            })
            
            print(f"   üìä Score obtido: {score:.1f}/10")
            
            # Verifica aprova√ß√£o
            if "APROVADO" in critic_result.content and "REQUER REVIS√ÉO MAIOR" not in critic_result.content:
                if score >= self.min_quality_score:
                    final_approved = True
                    print(f"   ‚úÖ Texto aprovado na itera√ß√£o {iteration}!")
                else:
                    print(f"   ‚ö†Ô∏è Score baixo ({score:.1f}) - continuando...")
            else:
                print(f"   üîÑ Requer revis√£o - continuando...")
            
            iteration += 1
        
        end_time = datetime.now()
        
        # Resultado final
        result = {
            "topic": topic,
            "final_content": current_draft,
            "plan": plan_result.content,
            "research": research_result.content,
            "critic_history": critic_history,
            "final_score": critic_history[-1]['score'],
            "iterations_used": iteration - 1,
            "max_iterations": self.max_iterations,
            "approved": final_approved,
            "llm_calls": self.llm.call_count,
            "execution_time_seconds": (end_time - start_time).total_seconds(),
            "provider_info": {
                "type": self.llm.provider_type,
                "model": self.llm.model_name
            },
            "timestamp": datetime.now().isoformat()
        }
        
        self.execution_log.append(result)
        return result
    
    def _extract_score(self, critic_content: str) -> float:
        """Extrai score num√©rico da avalia√ß√£o cr√≠tica"""
        import re
        
        # Procura por padr√µes como "7.8/10", "Pontua√ß√£o: 8.5"
        patterns = [
            r'Pontua√ß√£o Geral[:\s]+(\d+\.?\d*)[/\s]?10',
            r'Score[:\s]+(\d+\.?\d*)[/\s]?10',
            r'(\d+\.?\d*)/10'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, critic_content, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except:
                    continue
        
        # Fallback: procura por "APROVADO" vs "REQUER REVIS√ÉO"
        if "APROVADO" in critic_content.upper() and "REQUER REVIS√ÉO" not in critic_content.upper():
            return 8.5
        elif "APROVADO COM REVIS√ïES" in critic_content.upper():
            return 7.0
        else:
            return 6.0
    
    def save_result(self, result: Dict, filename: Optional[str] = None) -> str:
        """Salva resultado em arquivo JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"autonowrite_{timestamp}.json"
        
        os.makedirs("results", exist_ok=True)
        filepath = os.path.join("results", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Resultado salvo em: {filepath}")
        return filepath


class ExperimentRunner:
    """Execu√ß√£o de experimentos para TCC"""
    
    def __init__(self, system: AutonoWriteSystem):
        self.system = system
    
    def run_comparative_experiment(self, topics: List[str], iterations_list: List[int] = [1, 2, 3]) -> Dict:
        """Executa experimento comparativo"""
        
        experiment_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        results = {
            "experiment_id": experiment_id,
            "topics": topics,
            "configurations": [],
            "start_time": datetime.now().isoformat()
        }
        
        print(f"\nüß™ Experimento Comparativo: {experiment_id}")
        print(f"üìã T√≥picos: {len(topics)} | Configura√ß√µes: {iterations_list}")
        
        for max_iter in iterations_list:
            print(f"\n--- Configura√ß√£o: {max_iter} itera√ß√£o(√µes) ---")
            
            config_results = []
            total_calls_before = self.system.llm.call_count
            
            for i, topic in enumerate(topics, 1):
                print(f"\nüî¨ Teste {i}/{len(topics)}: {topic[:50]}...")
                
                calls_before = self.system.llm.call_count
                result = self.system.generate_content(topic, max_iter)
                calls_used = self.system.llm.call_count - calls_before
                
                test_result = {
                    "topic": topic,
                    "final_score": result["final_score"],
                    "iterations_used": result["iterations_used"],
                    "execution_time": result["execution_time_seconds"],
                    "llm_calls": calls_used,
                    "approved": result["approved"],
                    "content_length": len(result["final_content"])
                }
                
                config_results.append(test_result)
                print(f"   ‚úÖ Score: {result['final_score']:.1f} | Tempo: {result['execution_time_seconds']:.1f}s")
            
            # Estat√≠sticas da configura√ß√£o
            avg_score = sum(r["final_score"] for r in config_results) / len(config_results)
            avg_time = sum(r["execution_time"] for r in config_results) / len(config_results)
            total_calls = self.system.llm.call_count - total_calls_before
            
            config_summary = {
                "max_iterations": max_iter,
                "results": config_results,
                "statistics": {
                    "avg_score": round(avg_score, 2),
                    "avg_time": round(avg_time, 2),
                    "total_llm_calls": total_calls,
                    "approval_rate": sum(1 for r in config_results if r["approved"]) / len(config_results)
                }
            }
            
            results["configurations"].append(config_summary)
            print(f"üìä Resumo: Score m√©dio {avg_score:.1f} | Tempo m√©dio {avg_time:.1f}s | Aprova√ß√£o {config_summary['statistics']['approval_rate']:.1%}")
        
        results["end_time"] = datetime.now().isoformat()
        results["summary"] = self._generate_summary(results)
        
        # Salva experimento
        self.save_experiment(results)
        return results
    
    def _generate_summary(self, results: Dict) -> Dict:
        """Gera sum√°rio estat√≠stico"""
        configs = results["configurations"]
        
        best_quality = max(configs, key=lambda x: x["statistics"]["avg_score"])
        fastest = min(configs, key=lambda x: x["statistics"]["avg_time"])
        most_efficient = max(configs, key=lambda x: x["statistics"]["avg_score"] / x["statistics"]["avg_time"])
        
        scores = [c["statistics"]["avg_score"] for c in configs]
        quality_improvement = ((max(scores) - min(scores)) / min(scores)) * 100
        
        return {
            "best_quality_config": {
                "iterations": best_quality["max_iterations"],
                "avg_score": best_quality["statistics"]["avg_score"],
                "avg_time": best_quality["statistics"]["avg_time"]
            },
            "fastest_config": {
                "iterations": fastest["max_iterations"],
                "avg_score": fastest["statistics"]["avg_score"],
                "avg_time": fastest["statistics"]["avg_time"]
            },
            "most_efficient_config": {
                "iterations": most_efficient["max_iterations"],
                "avg_score": most_efficient["statistics"]["avg_score"],
                "avg_time": most_efficient["statistics"]["avg_time"],
                "efficiency_ratio": most_efficient["statistics"]["avg_score"] / most_efficient["statistics"]["avg_time"]
            },
            "quality_improvement_percent": round(quality_improvement, 1),
            "total_llm_calls": sum(c["statistics"]["total_llm_calls"] for c in configs),
            "recommendations": self._generate_recommendations(configs)
        }
    
    def _generate_recommendations(self, configs: List[Dict]) -> List[str]:
        """Gera recomenda√ß√µes baseadas nos resultados"""
        recommendations = []
        
        scores = [(c["max_iterations"], c["statistics"]["avg_score"]) for c in configs]
        scores.sort()
        
        # Analisa se h√° melhoria significativa
        if len(scores) > 1:
            improvement_1_to_2 = scores[1][1] - scores[0][1] if len(scores) > 1 else 0
            improvement_2_to_3 = scores[2][1] - scores[1][1] if len(scores) > 2 else 0
            
            if improvement_1_to_2 > 0.5:
                recommendations.append("Itera√ß√£o adicional produz melhoria significativa na qualidade")
            
            if len(scores) > 2 and improvement_2_to_3 < improvement_1_to_2 * 0.5:
                recommendations.append("Retornos decrescentes observados ap√≥s 2 itera√ß√µes")
            
            best_score_config = max(configs, key=lambda x: x["statistics"]["avg_score"])
            if best_score_config["statistics"]["avg_score"] >= 8.0:
                recommendations.append(f"Configura√ß√£o de {best_score_config['max_iterations']} itera√ß√µes atinge qualidade excelente")
        
        # Analisa efici√™ncia
        fastest = min(configs, key=lambda x: x["statistics"]["avg_time"])
        if fastest["statistics"]["avg_score"] >= 7.0:
            recommendations.append(f"Para uso r√°pido: {fastest['max_iterations']} itera√ß√£o(√µes) oferece boa qualidade em {fastest['statistics']['avg_time']:.1f}s")
        
        return recommendations
    
    def save_experiment(self, experiment: Dict, filename: str = None):
        """Salva experimento"""
        if filename is None:
            filename = f"experiment_{experiment['experiment_id']}.json"
        
        os.makedirs("experiments", exist_ok=True)
        filepath = os.path.join("experiments", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(experiment, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ Experimento salvo: {filepath}")
        return filepath
    
    def generate_tcc_report(self, experiment: Dict) -> str:
        """Gera relat√≥rio para TCC"""
        summary = experiment["summary"]
        
        report = f"""# Relat√≥rio Experimental - AutonoWrite

## Configura√ß√£o do Experimento
- **ID**: {experiment['experiment_id']}
- **T√≥picos Testados**: {len(experiment['topics'])}
- **Configura√ß√µes**: {[c['max_iterations'] for c in experiment['configurations']]} itera√ß√µes
- **Total de Testes**: {sum(len(c['results']) for c in experiment['configurations'])}

## Resultados Principais

### Melhor Qualidade
- **Configura√ß√£o**: {summary['best_quality_config']['iterations']} itera√ß√£o(√µes)
- **Score M√©dio**: {summary['best_quality_config']['avg_score']}/10
- **Tempo M√©dio**: {summary['best_quality_config']['avg_time']:.1f}s

### Mais R√°pida
- **Configura√ß√£o**: {summary['fastest_config']['iterations']} itera√ß√£o(√µes)
- **Score M√©dio**: {summary['fastest_config']['avg_score']}/10
- **Tempo M√©dio**: {summary['fastest_config']['avg_time']:.1f}s

### Mais Eficiente (Qualidade/Tempo)
- **Configura√ß√£o**: {summary['most_efficient_config']['iterations']} itera√ß√£o(√µes)
- **Raz√£o Efici√™ncia**: {summary['most_efficient_config']['efficiency_ratio']:.2f}

## An√°lises Estat√≠sticas

### Melhoria de Qualidade
- **Melhoria Total**: {summary['quality_improvement_percent']}%
- **Chamadas LLM Totais**: {summary['total_llm_calls']}

## Resultados Detalhados por Configura√ß√£o
"""
        
        for config in experiment['configurations']:
            stats = config['statistics']
            report += f"""
### {config['max_iterations']} Itera√ß√£o(√µes)
- **Score M√©dio**: {stats['avg_score']}/10
- **Tempo M√©dio**: {stats['avg_time']:.1f}s
- **Taxa de Aprova√ß√£o**: {stats['approval_rate']:.1%}
- **Chamadas LLM**: {stats['total_llm_calls']}

**T√≥picos Testados**: {len(config['results'])}
"""
            
            for result in config['results']:
                report += f"- {result['topic'][:60]}... (Score: {result['final_score']:.1f})\n"
        
        report += f"""
## Recomenda√ß√µes

{chr(10).join(f"- {rec}" for rec in summary['recommendations'])}

## Conclus√µes para o TCC

1. **Valida√ß√£o do Conceito**: O sistema multiagente com ciclo cr√≠tico demonstra melhoria mensur√°vel de qualidade
2. **Efic√°cia das Itera√ß√µes**: Itera√ß√µes adicionais produzem melhorias at√© um ponto √≥timo
3. **Aplicabilidade Pr√°tica**: Sistema vi√°vel para produ√ß√£o de conte√∫do automatizado de qualidade

---
*Relat√≥rio gerado automaticamente em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report


def main():
    """Fun√ß√£o principal - interface de linha de comando"""
    
    print("=" * 60)
    print("ü§ñ AutonoWrite - Sistema Multiagente para Gera√ß√£o de Conte√∫do")
    print("=" * 60)
    
    # Configura√ß√£o do LLM
    print("\nüîß Configurando sistema...")
    try:
        llm = LLMProvider("auto")
        system = AutonoWriteSystem(llm)
        print("‚úÖ Sistema configurado com sucesso!")
    except Exception as e:
        print(f"‚ùå Erro na configura√ß√£o: {e}")
        print("üí° Dica: Configure GROQ_API_KEY no arquivo .env ou use simula√ß√£o")
        return
    
    # Menu principal
    while True:
        print("\n" + "=" * 40)
        print("MENU PRINCIPAL")
        print("=" * 40)
        print("1. üìù Gerar conte√∫do √∫nico")
        print("2. üß™ Executar experimento comparativo")
        print("3. üìä Ver hist√≥rico de execu√ß√µes") 
        print("4. ‚öôÔ∏è Configura√ß√µes")
        print("5. üö™ Sair")
        
        choice = input("\nEscolha uma op√ß√£o (1-5): ").strip()
        
        if choice == "1":
            generate_single_content(system)
        elif choice == "2":
            run_experiment_menu(system)
        elif choice == "3":
            show_history(system)
        elif choice == "4":
            show_config_menu(system)
        elif choice == "5":
            print("\nüëã Obrigado por usar o AutonoWrite!")
            break
        else:
            print("‚ùå Op√ß√£o inv√°lida. Tente novamente.")


def generate_single_content(system: AutonoWriteSystem):
    """Gera conte√∫do √∫nico"""
    print("\nüìù GERA√á√ÉO DE CONTE√öDO √öNICO")
    print("-" * 30)
    
    # Input do t√≥pico
    topic = input("Digite o t√≥pico: ").strip()
    if not topic:
        print("‚ùå T√≥pico n√£o pode estar vazio")
        return
    
    # Configura√ß√£o de itera√ß√µes
    max_iter = input(f"M√°ximo de itera√ß√µes (atual: {system.max_iterations}): ").strip()
    if max_iter.isdigit():
        max_iter = int(max_iter)
    else:
        max_iter = system.max_iterations
    
    # Execu√ß√£o
    try:
        result = system.generate_content(topic, max_iter)
        
        # Resultados
        print("\n" + "=" * 50)
        print("üìä RESULTADOS")
        print("=" * 50)
        print(f"T√≥pico: {result['topic']}")
        print(f"Score Final: {result['final_score']:.1f}/10")
        print(f"Itera√ß√µes: {result['iterations_used']}/{result['max_iterations']}")
        print(f"Status: {'‚úÖ APROVADO' if result['approved'] else '‚ö†Ô∏è N√ÉO APROVADO'}")
        print(f"Tempo: {result['execution_time_seconds']:.1f}s")
        print(f"Chamadas LLM: {result['llm_calls']}")
        
        # Salvar resultado
        save = input("\nSalvar resultado? (s/N): ").strip().lower()
        if save in ['s', 'sim', 'y', 'yes']:
            filepath = system.save_result(result)
            
            # Mostrar conte√∫do
            show_content = input("Mostrar conte√∫do gerado? (s/N): ").strip().lower()
            if show_content in ['s', 'sim', 'y', 'yes']:
                print("\n" + "=" * 50)
                print("üìÑ CONTE√öDO GERADO")
                print("=" * 50)
                print(result['final_content'])
                print("=" * 50)
    
    except Exception as e:
        print(f"‚ùå Erro durante execu√ß√£o: {e}")


def run_experiment_menu(system: AutonoWriteSystem):
    """Menu de experimentos"""
    print("\nüß™ EXPERIMENTOS COMPARATIVOS")
    print("-" * 30)
    
    # T√≥picos predefinidos para TCC
    default_topics = [
        "Sistemas Multiagente em Intelig√™ncia Artificial",
        "Engenharia de Prompt para Modelos de Linguagem",
        "Ciclos de Cr√≠tica e Refinamento em IA Generativa",
        "Framework CrewAI para Automa√ß√£o de Tarefas",
        "Avalia√ß√£o de Qualidade em Gera√ß√£o Autom√°tica de Texto"
    ]
    
    print("Op√ß√µes:")
    print("1. Usar t√≥picos padr√£o (recomendado para TCC)")
    print("2. Definir t√≥picos customizados")
    
    choice = input("Escolha (1-2): ").strip()
    
    if choice == "1":
        topics = default_topics
        print(f"‚úÖ Usando {len(topics)} t√≥picos padr√£o")
    elif choice == "2":
        topics = []
        print("Digite os t√≥picos (Enter vazio para finalizar):")
        while True:
            topic = input(f"T√≥pico {len(topics)+1}: ").strip()
            if not topic:
                break
            topics.append(topic)
        
        if not topics:
            print("‚ùå Nenhum t√≥pico definido")
            return
    else:
        print("‚ùå Op√ß√£o inv√°lida")
        return
    
    # Configura√ß√µes do experimento
    iterations_input = input("Configura√ß√µes de itera√ß√µes (ex: 1,2,3): ").strip()
    if iterations_input:
        try:
            iterations_list = [int(x.strip()) for x in iterations_input.split(',')]
        except:
            iterations_list = [1, 2, 3]
    else:
        iterations_list = [1, 2, 3]
    
    print(f"üîß Configura√ß√µes:")
    print(f"   - T√≥picos: {len(topics)}")
    print(f"   - Itera√ß√µes: {iterations_list}")
    print(f"   - Total de testes: {len(topics) * len(iterations_list)}")
    
    confirm = input("\nExecutar experimento? (s/N): ").strip().lower()
    if confirm not in ['s', 'sim', 'y', 'yes']:
        return
    
    # Execu√ß√£o do experimento
    try:
        runner = ExperimentRunner(system)
        result = runner.run_comparative_experiment(topics, iterations_list)
        
        # Gerar e salvar relat√≥rio
        report = runner.generate_tcc_report(result)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"relatorio_tcc_{timestamp}.md"
        
        os.makedirs("reports", exist_ok=True)
        report_path = os.path.join("reports", report_file)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nüìä Experimento conclu√≠do!")
        print(f"üìÑ Relat√≥rio salvo: {report_path}")
        
        # Mostrar sum√°rio
        summary = result["summary"]
        print(f"\nüéØ RESUMO EXECUTIVO:")
        print(f"   - Melhor qualidade: {summary['best_quality_config']['iterations']} iter (Score: {summary['best_quality_config']['avg_score']:.1f})")
        print(f"   - Mais r√°pido: {summary['fastest_config']['iterations']} iter ({summary['fastest_config']['avg_time']:.1f}s)")
        print(f"   - Melhoria total: {summary['quality_improvement_percent']}%")
        
    except Exception as e:
        print(f"‚ùå Erro no experimento: {e}")


def show_history(system: AutonoWriteSystem):
    """Mostra hist√≥rico de execu√ß√µes"""
    if not system.execution_log:
        print("\nüìã Nenhuma execu√ß√£o no hist√≥rico atual")
        return
    
    print(f"\nüìã HIST√ìRICO ({len(system.execution_log)} execu√ß√µes)")
    print("-" * 50)
    
    for i, result in enumerate(system.execution_log, 1):
        print(f"{i}. {result['topic'][:40]}...")
        print(f"   Score: {result['final_score']:.1f} | Iter: {result['iterations_used']} | Tempo: {result['execution_time_seconds']:.1f}s")
        print()


def show_config_menu(system: AutonoWriteSystem):
    """Menu de configura√ß√µes"""
    print(f"\n‚öôÔ∏è CONFIGURA√á√ïES ATUAIS")
    print("-" * 30)
    print(f"Provider: {system.llm.provider_type}")
    print(f"Modelo: {system.llm.model_name}")
    print(f"Max Itera√ß√µes: {system.max_iterations}")
    print(f"Score M√≠nimo: {system.min_quality_score}")
    print(f"Chamadas LLM: {system.llm.call_count}")
    
    print(f"\nOp√ß√µes:")
    print(f"1. Alterar m√°ximo de itera√ß√µes")
    print(f"2. Alterar score m√≠nimo")
    print(f"3. Resetar contador de chamadas")
    print(f"4. Voltar")
    
    choice = input("Escolha (1-4): ").strip()
    
    if choice == "1":
        new_max = input(f"Novo m√°ximo de itera√ß√µes (atual: {system.max_iterations}): ").strip()
        if new_max.isdigit() and int(new_max) > 0:
            system.max_iterations = int(new_max)
            print(f"‚úÖ M√°ximo de itera√ß√µes alterado para {system.max_iterations}")
        else:
            print("‚ùå Valor inv√°lido")
    
    elif choice == "2":
        new_score = input(f"Novo score m√≠nimo (atual: {system.min_quality_score}): ").strip()
        try:
            score = float(new_score)
            if 0 <= score <= 10:
                system.min_quality_score = score
                print(f"‚úÖ Score m√≠nimo alterado para {system.min_quality_score}")
            else:
                print("‚ùå Score deve estar entre 0 e 10")
        except:
            print("‚ùå Valor inv√°lido")
    
    elif choice == "3":
        system.llm.call_count = 0
        print("‚úÖ Contador resetado")


if __name__ == "__main__":
    main()