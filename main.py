#!/usr/bin/env python3
"""
AutonoWrite - Sistema Multiagente para GeraÃ§Ã£o de ConteÃºdo
VersÃ£o integrada para TCC com recursos gratuitos
"""

import os
import json
import time
from typing import Dict, List, Optional
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("âš ï¸  Groq nÃ£o instalado. Use: pip install groq")

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

try:
    from dotenv import load_dotenv
    load_dotenv()
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False


class LLMProvider:
    """Provider base para diferentes modelos LLM"""
    
    def __init__(self, provider_type: str = "simulation"):
        self.call_count = 0
        self.provider_type = provider_type
        self.client = None
        
        if provider_type == "auto":
            self._setup_auto()
        elif provider_type == "groq":
            self._setup_groq()
        elif provider_type == "ollama":
            self._setup_ollama()
        elif provider_type == "simulation":
            self._setup_simulation()
    
    def _setup_auto(self):
        """Setup automÃ¡tico - prioriza modo de simulaÃ§Ã£o para testes"""
        print("âš ï¸  Usando modo de simulaÃ§Ã£o por padrÃ£o")
        self._setup_simulation()
    
    def _setup_groq(self):
        """Configura Groq API"""
        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            raise ValueError("GROQ_API_KEY nÃ£o encontrada. Configure no .env")
        
        self.client = Groq(api_key=api_key)
        self.provider_type = "groq"
        self.model_name = "mixtral-8x7b-32768"
        print("ğŸš€ Usando Groq API (mixtral-8x7b-32768)")
    
    def _setup_ollama(self):
        """Configura Ollama local"""
        self.provider_type = "ollama"
        self.model_name = "gemma:2b"  # Modelo mais leve para sistemas com menos memÃ³ria
        print(f"ğŸ  Usando Ollama local ({self.model_name})")
        print("â„¹ï¸  Baixando o modelo pela primeira vez, pode demorar alguns minutos...")
        try:
            import ollama
            ollama.pull(self.model_name)  # Garante que o modelo estÃ¡ baixado
        except Exception as e:
            print(f"âš ï¸  Erro ao baixar o modelo: {e}")
            raise
    
    def _setup_simulation(self):
        """Configura simulaÃ§Ã£o para desenvolvimento"""
        self.provider_type = "simulation"
        self.model_name = "simulado"
        print("ğŸ­ Usando simulaÃ§Ã£o (para desenvolvimento)")
        print("â„¹ï¸  Modo de simulaÃ§Ã£o ativado - usando respostas prÃ©-definidas")
    
    def generate(self, prompt: str, max_tokens: int = 1500) -> str:
        """Gera resposta usando o provider configurado"""
        self.call_count += 1
        
        if self.provider_type == "groq":
            return self._generate_groq(prompt, max_tokens)
        elif self.provider_type == "ollama":
            return self._generate_ollama(prompt, max_tokens)
        else:
            return self._generate_simulation(prompt)
    
    def _generate_groq(self, prompt: str, max_tokens: int) -> str:
        """GeraÃ§Ã£o via Groq"""
        try:
            completion = self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model_name,
                max_tokens=max_tokens,
                temperature=0.7
            )
            return completion.choices[0].message.content
        except Exception as e:
            return f"Erro Groq: {str(e)}"
    
    def _generate_ollama(self, prompt: str, max_tokens: int) -> str:
        """GeraÃ§Ã£o via Ollama"""
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[{'role': 'user', 'content': prompt}]
            )
            return response['message']['content']
        except Exception as e:
            return f"Erro Ollama: {str(e)}"
    
    def _simulate_llm_response(self, prompt: str, max_tokens: int) -> str:
        """Simula uma resposta do LLM para desenvolvimento"""
        # Simula diferentes tipos de respostas baseado no conteÃºdo do prompt
        if "planejamento" in prompt.lower() or "plano" in prompt.lower():
            return """
            Plano para o tÃ³pico:
            1. IntroduÃ§Ã£o ao conceito
            2. ExplicaÃ§Ã£o detalhada
            3. Exemplos prÃ¡ticos
            4. ConclusÃ£o
            """
        elif "pesquisa" in prompt.lower():
            return """
            Pesquisa sobre o tÃ³pico:
            - Fonte 1: Artigo cientÃ­fico relevante
            - Fonte 2: Dados estatÃ­sticos atualizados
            - Fonte 3: Estudos de caso
            """
        elif "avaliaÃ§Ã£o" in prompt.lower() or "crÃ­tica" in prompt.lower():
            return """
            AvaliaÃ§Ã£o crÃ­tica:
            - Pontos fortes: Clareza, organizaÃ§Ã£o
            - Pontos fracos: Poderia ter mais exemplos
            - PontuaÃ§Ã£o: 8.5/10
            """
        elif "crÃ­tico" in prompt.lower() or "editor" in prompt.lower():
            return self._simulate_critic()
        else:
            return f"SimulaÃ§Ã£o - Resposta para: {prompt[:100]}..."
    
    def _simulate_planner(self) -> str:
        return """
## Plano Estruturado

### 1. IntroduÃ§Ã£o
- ContextualizaÃ§Ã£o do tema
- RelevÃ¢ncia e importÃ¢ncia atual
- Objetivos do artigo

### 2. FundamentaÃ§Ã£o TeÃ³rica  
- Conceitos fundamentais
- Estado da arte
- RevisÃ£o da literatura

### 3. AnÃ¡lise CrÃ­tica
- Vantagens e limitaÃ§Ãµes
- Casos de uso prÃ¡ticos
- ComparaÃ§Ãµes com alternativas

### 4. DiscussÃ£o e ImplicaÃ§Ãµes
- Impactos na Ã¡rea
- Desafios identificados
- Oportunidades futuras

### 5. ConclusÃ£o
- SÃ­ntese dos pontos principais
- ContribuiÃ§Ãµes do trabalho
- SugestÃµes para pesquisas futuras

**Palavras-chave**: sistema multiagente, inteligÃªncia artificial, automaÃ§Ã£o
"""
    
    def _simulate_researcher(self) -> str:
        return """
## RelatÃ³rio de Pesquisa

### Fontes Identificadas:
1. "Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations" (2008)
2. "Large Language Models: A Survey" - ArXiv 2024  
3. "CrewAI Framework Documentation" - DocumentaÃ§Ã£o oficial 2024
4. "Prompt Engineering for Large Language Models" - ACM 2024

### Dados Relevantes:
- Sistemas multiagente mostram 40-60% de melhoria em tarefas complexas
- LLMs apresentam taxa de alucinaÃ§Ã£o de 15-25% em tarefas factuais
- Ciclos de crÃ­tica reduzem alucinaÃ§Ãµes em atÃ© 70%
- CrewAI framework adotado por 500+ empresas em 2024

### InformaÃ§Ãµes TÃ©cnicas:
- Arquitetura baseada em agentes especializados
- Uso de engenharia de prompt para orquestraÃ§Ã£o
- ImplementaÃ§Ã£o de ciclos de feedback e refinamento
- MÃ©tricas de avaliaÃ§Ã£o: ROUGE, BLEU, avaliaÃ§Ã£o humana
"""
    
    def _simulate_writer(self) -> str:
        return """
# Sistemas Multiagente para GeraÃ§Ã£o Automatizada de ConteÃºdo

## IntroduÃ§Ã£o

A evoluÃ§Ã£o da inteligÃªncia artificial tem proporcionado avanÃ§os significativos na automaÃ§Ã£o de tarefas cognitivas complexas. Entre essas inovaÃ§Ãµes, os sistemas multiagente emergem como uma abordagem promissora para superar limitaÃ§Ãµes dos modelos monolÃ­ticos tradicionais, especialmente no contexto de geraÃ§Ã£o de conteÃºdo.

## FundamentaÃ§Ã£o TeÃ³rica

Os sistemas multiagente (MAS) representam uma arquitetura distribuÃ­da onde mÃºltiplos agentes autÃ´nomos colaboram para resolver problemas complexos. No contexto de geraÃ§Ã£o de texto, essa abordagem permite a especializaÃ§Ã£o de diferentes componentes: planejamento, pesquisa, redaÃ§Ã£o e revisÃ£o.

A filosofia "dividir para conquistar" permite decompor a tarefa de escrita em subtarefas menores e mais gerenciÃ¡veis, cada uma executada por um agente especializado com objetivos e competÃªncias especÃ­ficas.

## AnÃ¡lise CrÃ­tica

A principal vantagem dos sistemas multiagente reside na capacidade de emular processos humanos de produÃ§Ã£o editorial. Um escritor profissional naturalmente alterna entre fases de pesquisa, estruturaÃ§Ã£o, redaÃ§Ã£o e revisÃ£o. O AutonoWrite replica esse processo atravÃ©s de agentes especializados.

### Vantagens Identificadas:
- ReduÃ§Ã£o significativa de alucinaÃ§Ãµes atravÃ©s do ciclo crÃ­tico
- Melhoria iterativa da qualidade do conteÃºdo
- EspecializaÃ§Ã£o permite maior profundidade tÃ©cnica
- Flexibilidade para diferentes tipos de conteÃºdo

### LimitaÃ§Ãµes Observadas:
- Maior custo computacional devido a mÃºltiplas iteraÃ§Ãµes
- DependÃªncia da qualidade dos prompts de cada agente
- Complexidade de orquestraÃ§Ã£o entre componentes

## ConclusÃ£o

O sistema AutonoWrite demonstra o potencial dos sistemas multiagente para superar limitaÃ§Ãµes dos modelos monolÃ­ticos. AtravÃ©s da especializaÃ§Ã£o e colaboraÃ§Ã£o entre agentes, Ã© possÃ­vel produzir conteÃºdo de maior qualidade, factualidade e coerÃªncia.

Os resultados preliminares indicam que a abordagem multiagente representa um avanÃ§o significativo na automaÃ§Ã£o de tarefas de conhecimento, com aplicaÃ§Ãµes promissoras em contextos acadÃªmicos, jornalÃ­sticos e corporativos.
"""
    
    def _simulate_critic(self) -> str:
        return """
## AvaliaÃ§Ã£o CrÃ­tica do ConteÃºdo

### PontuaÃ§Ã£o Geral: 7.8/10

### CritÃ©rios Detalhados:

**Estrutura e OrganizaÃ§Ã£o: 8.5/10**
âœ… Estrutura lÃ³gica clara com introduÃ§Ã£o, desenvolvimento e conclusÃ£o
âœ… TransiÃ§Ãµes adequadas entre seÃ§Ãµes  
âš ï¸ Algumas seÃ§Ãµes poderiam ser mais equilibradas

**Profundidade TÃ©cnica: 7.0/10**
âœ… Conceitos fundamentais bem explicados
âœ… Terminologia tÃ©cnica apropriada
âš ï¸ Poderia incluir mais exemplos prÃ¡ticos na seÃ§Ã£o de anÃ¡lise

**Clareza e Legibilidade: 8.5/10**
âœ… Linguagem clara e acessÃ­vel
âœ… Fluxo narrativo coerente
âœ… ParÃ¡grafos bem construÃ­dos

**Verificabilidade das Fontes: 6.5/10** 
âš ï¸ Menciona conceitos sem citaÃ§Ãµes especÃ­ficas
âš ï¸ Faltam referÃªncias diretas a estudos mencionados
âš ï¸ NecessÃ¡rio incluir mais dados quantitativos

**CoerÃªncia Argumentativa: 8.0/10**
âœ… Argumentos consistentes ao longo do texto
âœ… ConclusÃµes coerentes com o desenvolvimento
âš ï¸ Poderia fortalecer contrapontos

### RecomendaÃ§Ãµes EspecÃ­ficas:
1. Adicionar citaÃ§Ãµes diretas para os dados quantitativos mencionados
2. Incluir exemplo prÃ¡tico de implementaÃ§Ã£o na seÃ§Ã£o 3
3. Expandir discussÃ£o sobre limitaÃ§Ãµes Ã©ticas
4. Verificar se todos os termos tÃ©cnicos estÃ£o adequadamente definidos

**Status: APROVADO COM REVISÃ•ES MENORES**"""
        
    def _generate_simulation(self, prompt: str) -> str:
        """Gera uma resposta simulada baseada no papel do agente"""
        # Simula diferentes tipos de respostas baseado no papel do agente
        if "planejador" in prompt.lower() or "plano" in prompt.lower():
            return self._simulate_planner()
        elif "pesquisador" in prompt.lower() or "pesquisa" in prompt.lower():
            return self._simulate_researcher()
        elif "redator" in prompt.lower() or "escrever" in prompt.lower():
            return self._simulate_writer()
        elif "crÃ­tico" in prompt.lower() or "avaliar" in prompt.lower():
            return self._simulate_critic()
        else:
            # Resposta genÃ©rica para outros casos
            return self._simulate_llm_response(prompt, max_tokens=1000)


class AgentRole(Enum):
    PLANNER = "planejador"
    RESEARCHER = "pesquisador"
    WRITER = "redator"
    CRITIC = "crÃ­tico"


@dataclass
class TaskResult:
    agent_role: AgentRole
    content: str
    metadata: Dict
    timestamp: datetime
    success: bool = True


class Agent:
    """Agente especializado do sistema"""
    
    def __init__(self, role: AgentRole, llm_provider: LLMProvider):
        self.role = role
        self.llm = llm_provider
        self.memory = []
    
    def execute_task(self, task: str, context: Optional[str] = None) -> TaskResult:
        """Executa tarefa especÃ­fica do agente"""
        
        prompt = self._build_prompt(task, context)
        response = self.llm.generate(prompt)
        
        result = TaskResult(
            agent_role=self.role,
            content=response,
            metadata={
                "prompt_length": len(prompt),
                "response_length": len(response),
                "context_provided": context is not None
            },
            timestamp=datetime.now()
        )
        
        self.memory.append(result)
        return result
    
    def _build_prompt(self, task: str, context: Optional[str] = None) -> str:
        """ConstrÃ³i prompt especÃ­fico para cada agente"""
        
        prompts = {
            AgentRole.PLANNER: f"""
VocÃª Ã© um PLANEJADOR ESTRATÃ‰GICO especialista em estruturaÃ§Ã£o de conteÃºdo acadÃªmico e tÃ©cnico.

TAREFA: {task}

Crie um plano detalhado e bem estruturado para abordar este tÃ³pico. Inclua:
- SeÃ§Ãµes principais e subseÃ§Ãµes
- Pontos-chave a serem abordados
- SequÃªncia lÃ³gica de desenvolvimento
- Palavras-chave relevantes

Formate como um outline claro e hierÃ¡rquico.
""",
            
            AgentRole.RESEARCHER: f"""
VocÃª Ã© um PESQUISADOR ACADÃŠMICO meticuloso e experiente.

TAREFA: {task}
PLANO DE REFERÃŠNCIA: {context or 'NÃ£o fornecido'}

Conduza uma pesquisa abrangente sobre o tÃ³pico. ForneÃ§a:
- Fontes confiÃ¡veis e atuais (artigos, estudos, documentaÃ§Ã£o)
- Dados quantitativos relevantes
- InformaÃ§Ãµes tÃ©cnicas precisas
- CitaÃ§Ãµes e referÃªncias quando apropriado

Organize as informaÃ§Ãµes de forma clara e estruturada.
""",
            
            AgentRole.WRITER: f"""
VocÃª Ã© um REDATOR PROFISSIONAL com expertise em conteÃºdo tÃ©cnico e acadÃªmico.

TAREFA: {task}
CONTEXTO DISPONÃVEL: {context or 'NÃ£o fornecido'}

Escreva um texto completo, bem estruturado e envolvente. Garanta:
- Linguagem clara e apropriada para o pÃºblico-alvo
- Estrutura lÃ³gica seguindo o plano fornecido
- Uso adequado das informaÃ§Ãµes de pesquisa
- TransiÃ§Ãµes suaves entre seÃ§Ãµes
- ConclusÃ£o que sintetiza os pontos principais

Produza um texto coerente e de alta qualidade.
""",
            
            AgentRole.CRITIC: f"""
VocÃª Ã© um CRÃTICO RIGOROSO e avaliador de qualidade editorial.

TEXTO PARA AVALIAÃ‡ÃƒO: {task}

Avalie o texto com base nos seguintes critÃ©rios:
1. **Estrutura e OrganizaÃ§Ã£o** (0-10)
2. **Profundidade TÃ©cnica** (0-10)  
3. **Clareza e Legibilidade** (0-10)
4. **Verificabilidade das Fontes** (0-10)
5. **CoerÃªncia Argumentativa** (0-10)

Para cada critÃ©rio:
- ForneÃ§a uma pontuaÃ§Ã£o
- Identifique pontos fortes
- Aponte Ã¡reas para melhoria
- OfereÃ§a sugestÃµes especÃ­ficas

Conclua com:
- **PontuaÃ§Ã£o Geral** (mÃ©dia ponderada)
- **Status**: APROVADO, APROVADO COM REVISÃ•ES, ou REQUER REVISÃƒO MAIOR
- **RecomendaÃ§Ãµes prioritÃ¡rias** para melhoria

Seja construtivo mas rigoroso na avaliaÃ§Ã£o.
"""
        }
        
        return prompts[self.role]


class AutonoWriteSystem:
    """Sistema principal AutonoWrite"""
    
    def __init__(self, llm_provider: LLMProvider):
        self.llm = llm_provider
        self.agents = {
            AgentRole.PLANNER: Agent(AgentRole.PLANNER, llm_provider),
            AgentRole.RESEARCHER: Agent(AgentRole.RESEARCHER, llm_provider),
            AgentRole.WRITER: Agent(AgentRole.WRITER, llm_provider),
            AgentRole.CRITIC: Agent(AgentRole.CRITIC, llm_provider)
        }
        self.max_iterations = 3
        self.min_quality_score = 8.0
        self.execution_log = []
    
    def generate_content(self, topic: str, max_iterations: Optional[int] = None) -> Dict:
        """Pipeline principal de geraÃ§Ã£o"""
        
        if max_iterations:
            self.max_iterations = max_iterations
        
        print(f"\nğŸš€ AutonoWrite iniciando para: {topic}")
        print(f"ğŸ“Š ConfiguraÃ§Ã£o: {self.max_iterations} iteraÃ§Ãµes mÃ¡ximas")
        print(f"ğŸ¤– Provider: {self.llm.provider_type} ({self.llm.model_name})")
        
        start_time = datetime.now()
        
        # Fase 1: Planejamento
        print(f"\nğŸ“‹ Fase 1: Planejamento...")
        plan_result = self.agents[AgentRole.PLANNER].execute_task(
            f"Criar plano estruturado para artigo sobre: {topic}"
        )
        
        # Fase 2: Pesquisa  
        print(f"\nğŸ” Fase 2: Pesquisa...")
        research_result = self.agents[AgentRole.RESEARCHER].execute_task(
            f"Pesquisar informaÃ§Ãµes detalhadas sobre: {topic}",
            context=plan_result.content
        )
        
        # Fase 3: RedaÃ§Ã£o com ciclo crÃ­tico
        print(f"\nâœï¸ Fase 3: RedaÃ§Ã£o e Refinamento...")
        
        current_draft = None
        iteration = 1
        final_approved = False
        critic_history = []
        
        while iteration <= self.max_iterations and not final_approved:
            print(f"\n   ğŸ“ IteraÃ§Ã£o {iteration}/{self.max_iterations}")
            
            # RedaÃ§Ã£o/revisÃ£o
            if current_draft is None:
                # Primeira redaÃ§Ã£o
                context = f"PLANO:\n{plan_result.content}\n\nPESQUISA:\n{research_result.content}"
                task = f"Escrever artigo completo sobre: {topic}"
            else:
                # RevisÃ£o baseada em feedback
                context = f"FEEDBACK ANTERIOR:\n{critic_history[-1]['content']}"
                task = f"Revisar e melhorar este texto:\n\n{current_draft}"
            
            write_result = self.agents[AgentRole.WRITER].execute_task(task, context)
            current_draft = write_result.content
            
            # AvaliaÃ§Ã£o crÃ­tica
            print(f"   ğŸ” AvaliaÃ§Ã£o crÃ­tica...")
            critic_result = self.agents[AgentRole.CRITIC].execute_task(current_draft)
            
            # Extrai score da avaliaÃ§Ã£o
            score = self._extract_score(critic_result.content)
            
            critic_history.append({
                'iteration': iteration,
                'content': critic_result.content,
                'score': score
            })
            
            print(f"   ğŸ“Š Score obtido: {score:.1f}/10")
            
            # Verifica aprovaÃ§Ã£o
            if "APROVADO" in critic_result.content and "REQUER REVISÃƒO MAIOR" not in critic_result.content:
                if score >= self.min_quality_score:
                    final_approved = True
                    print(f"   âœ… Texto aprovado na iteraÃ§Ã£o {iteration}!")
                else:
                    print(f"   âš ï¸ Score baixo ({score:.1f}) - continuando...")
            else:
                print(f"   ğŸ”„ Requer revisÃ£o - continuando...")
            
            iteration += 1
        
        end_time = datetime.now()
        
        # Resultado final
        result = {
            "topic": topic,
            "final_content": current_draft,
            "plan": plan_result.content,
            "research": research_result.content,
            "critic_history": critic_history,
            "final_score": critic_history[-1]['score'],
            "iterations_used": iteration - 1,
            "max_iterations": self.max_iterations,
            "approved": final_approved,
            "llm_calls": self.llm.call_count,
            "execution_time_seconds": (end_time - start_time).total_seconds(),
            "provider_info": {
                "type": self.llm.provider_type,
                "model": self.llm.model_name
            },
            "timestamp": datetime.now().isoformat()
        }
        
        self.execution_log.append(result)
        return result
    
    def _extract_score(self, critic_content: str) -> float:
        """Extrai score numÃ©rico da avaliaÃ§Ã£o crÃ­tica"""
        import re
        
        # Procura por padrÃµes como "7.8/10", "PontuaÃ§Ã£o: 8.5"
        patterns = [
            r'PontuaÃ§Ã£o Geral[:\s]+(\d+\.?\d*)[/\s]?10',
            r'Score[:\s]+(\d+\.?\d*)[/\s]?10',
            r'(\d+\.?\d*)/10'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, critic_content, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except:
                    continue
        
        # Fallback: procura por "APROVADO" vs "REQUER REVISÃƒO"
        if "APROVADO" in critic_content.upper() and "REQUER REVISÃƒO" not in critic_content.upper():
            return 8.5
        elif "APROVADO COM REVISÃ•ES" in critic_content.upper():
            return 7.0
        else:
            return 6.0
    
    def save_result(self, result: Dict, filename: Optional[str] = None) -> str:
        """Salva resultado em arquivo JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"autonowrite_{timestamp}.json"
        
        os.makedirs("results", exist_ok=True)
        filepath = os.path.join("results", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Resultado salvo em: {filepath}")
        return filepath


class ExperimentRunner:
    """ExecuÃ§Ã£o de experimentos para TCC"""
    
    def __init__(self, system: AutonoWriteSystem):
        self.system = system
    
    def run_comparative_experiment(self, topics: List[str], iterations_list: List[int] = [1, 2, 3]) -> Dict:
        """Executa experimento comparativo"""
        
        experiment_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        results = {
            "experiment_id": experiment_id,
            "topics": topics,
            "configurations": [],
            "start_time": datetime.now().isoformat()
        }
        
        print(f"\nğŸ§ª Experimento Comparativo: {experiment_id}")
        print(f"ğŸ“‹ TÃ³picos: {len(topics)} | ConfiguraÃ§Ãµes: {iterations_list}")
        
        for max_iter in iterations_list:
            print(f"\n--- ConfiguraÃ§Ã£o: {max_iter} iteraÃ§Ã£o(Ãµes) ---")
            
            config_results = []
            total_calls_before = self.system.llm.call_count
            
            for i, topic in enumerate(topics, 1):
                print(f"\nğŸ”¬ Teste {i}/{len(topics)}: {topic[:50]}...")
                
                calls_before = self.system.llm.call_count
                result = self.system.generate_content(topic, max_iter)
                calls_used = self.system.llm.call_count - calls_before
                
                test_result = {
                    "topic": topic,
                    "final_score": result["final_score"],
                    "iterations_used": result["iterations_used"],
                    "execution_time": result["execution_time_seconds"],
                    "llm_calls": calls_used,
                    "approved": result["approved"],
                    "content_length": len(result["final_content"])
                }
                
                config_results.append(test_result)
                print(f"   âœ… Score: {result['final_score']:.1f} | Tempo: {result['execution_time_seconds']:.1f}s")
            
            # EstatÃ­sticas da configuraÃ§Ã£o
            avg_score = sum(r["final_score"] for r in config_results) / len(config_results)
            avg_time = sum(r["execution_time"] for r in config_results) / len(config_results)
            total_calls = self.system.llm.call_count - total_calls_before
            
            config_summary = {
                "max_iterations": max_iter,
                "results": config_results,
                "statistics": {
                    "avg_score": round(avg_score, 2),
                    "avg_time": round(avg_time, 2),
                    "total_llm_calls": total_calls,
                    "approval_rate": sum(1 for r in config_results if r["approved"]) / len(config_results)
                }
            }
            
            results["configurations"].append(config_summary)
            print(f"ğŸ“Š Resumo: Score mÃ©dio {avg_score:.1f} | Tempo mÃ©dio {avg_time:.1f}s | AprovaÃ§Ã£o {config_summary['statistics']['approval_rate']:.1%}")
        
        results["end_time"] = datetime.now().isoformat()
        results["summary"] = self._generate_summary(results)
        
        # Salva experimento
        self.save_experiment(results)
        return results
    
    def _generate_summary(self, results: Dict) -> Dict:
        """Gera sumÃ¡rio estatÃ­stico"""
        configs = results["configurations"]
        
        best_quality = max(configs, key=lambda x: x["statistics"]["avg_score"])
        fastest = min(configs, key=lambda x: x["statistics"]["avg_time"])
        most_efficient = max(configs, key=lambda x: x["statistics"]["avg_score"] / x["statistics"]["avg_time"])
        
        scores = [c["statistics"]["avg_score"] for c in configs]
        quality_improvement = ((max(scores) - min(scores)) / min(scores)) * 100
        
        return {
            "best_quality_config": {
                "iterations": best_quality["max_iterations"],
                "avg_score": best_quality["statistics"]["avg_score"],
                "avg_time": best_quality["statistics"]["avg_time"]
            },
            "fastest_config": {
                "iterations": fastest["max_iterations"],
                "avg_score": fastest["statistics"]["avg_score"],
                "avg_time": fastest["statistics"]["avg_time"]
            },
            "most_efficient_config": {
                "iterations": most_efficient["max_iterations"],
                "avg_score": most_efficient["statistics"]["avg_score"],
                "avg_time": most_efficient["statistics"]["avg_time"],
                "efficiency_ratio": most_efficient["statistics"]["avg_score"] / most_efficient["statistics"]["avg_time"]
            },
            "quality_improvement_percent": round(quality_improvement, 1),
            "total_llm_calls": sum(c["statistics"]["total_llm_calls"] for c in configs),
            "recommendations": self._generate_recommendations(configs)
        }
    
    def _generate_recommendations(self, configs: List[Dict]) -> List[str]:
        """Gera recomendaÃ§Ãµes baseadas nos resultados"""
        recommendations = []
        
        scores = [(c["max_iterations"], c["statistics"]["avg_score"]) for c in configs]
        scores.sort()
        
        # Analisa se hÃ¡ melhoria significativa
        if len(scores) > 1:
            improvement_1_to_2 = scores[1][1] - scores[0][1] if len(scores) > 1 else 0
            improvement_2_to_3 = scores[2][1] - scores[1][1] if len(scores) > 2 else 0
            
            if improvement_1_to_2 > 0.5:
                recommendations.append("IteraÃ§Ã£o adicional produz melhoria significativa na qualidade")
            
            if len(scores) > 2 and improvement_2_to_3 < improvement_1_to_2 * 0.5:
                recommendations.append("Retornos decrescentes observados apÃ³s 2 iteraÃ§Ãµes")
            
            best_score_config = max(configs, key=lambda x: x["statistics"]["avg_score"])
            if best_score_config["statistics"]["avg_score"] >= 8.0:
                recommendations.append(f"ConfiguraÃ§Ã£o de {best_score_config['max_iterations']} iteraÃ§Ãµes atinge qualidade excelente")
        
        # Analisa eficiÃªncia
        fastest = min(configs, key=lambda x: x["statistics"]["avg_time"])
        if fastest["statistics"]["avg_score"] >= 7.0:
            recommendations.append(f"Para uso rÃ¡pido: {fastest['max_iterations']} iteraÃ§Ã£o(Ãµes) oferece boa qualidade em {fastest['statistics']['avg_time']:.1f}s")
        
        return recommendations
    
    def save_experiment(self, experiment: Dict, filename: str = None):
        """Salva experimento"""
        if filename is None:
            filename = f"experiment_{experiment['experiment_id']}.json"
        
        os.makedirs("experiments", exist_ok=True)
        filepath = os.path.join("experiments", filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(experiment, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Experimento salvo: {filepath}")
        return filepath
    
    def generate_tcc_report(self, experiment: Dict) -> str:
        """Gera relatÃ³rio para TCC"""
        summary = experiment["summary"]
        
        report = f"""# RelatÃ³rio Experimental - AutonoWrite

## ConfiguraÃ§Ã£o do Experimento
- **ID**: {experiment['experiment_id']}
- **TÃ³picos Testados**: {len(experiment['topics'])}
- **ConfiguraÃ§Ãµes**: {[c['max_iterations'] for c in experiment['configurations']]} iteraÃ§Ãµes
- **Total de Testes**: {sum(len(c['results']) for c in experiment['configurations'])}

## Resultados Principais

### Melhor Qualidade
- **ConfiguraÃ§Ã£o**: {summary['best_quality_config']['iterations']} iteraÃ§Ã£o(Ãµes)
- **Score MÃ©dio**: {summary['best_quality_config']['avg_score']}/10
- **Tempo MÃ©dio**: {summary['best_quality_config']['avg_time']:.1f}s

### Mais RÃ¡pida
- **ConfiguraÃ§Ã£o**: {summary['fastest_config']['iterations']} iteraÃ§Ã£o(Ãµes)
- **Score MÃ©dio**: {summary['fastest_config']['avg_score']}/10
- **Tempo MÃ©dio**: {summary['fastest_config']['avg_time']:.1f}s

### Mais Eficiente (Qualidade/Tempo)
- **ConfiguraÃ§Ã£o**: {summary['most_efficient_config']['iterations']} iteraÃ§Ã£o(Ãµes)
- **RazÃ£o EficiÃªncia**: {summary['most_efficient_config']['efficiency_ratio']:.2f}

## AnÃ¡lises EstatÃ­sticas

### Melhoria de Qualidade
- **Melhoria Total**: {summary['quality_improvement_percent']}%
- **Chamadas LLM Totais**: {summary['total_llm_calls']}

## Resultados Detalhados por ConfiguraÃ§Ã£o
"""
        
        for config in experiment['configurations']:
            stats = config['statistics']
            report += f"""
### {config['max_iterations']} IteraÃ§Ã£o(Ãµes)
- **Score MÃ©dio**: {stats['avg_score']}/10
- **Tempo MÃ©dio**: {stats['avg_time']:.1f}s
- **Taxa de AprovaÃ§Ã£o**: {stats['approval_rate']:.1%}
- **Chamadas LLM**: {stats['total_llm_calls']}

**TÃ³picos Testados**: {len(config['results'])}
"""
            
            for result in config['results']:
                report += f"- {result['topic'][:60]}... (Score: {result['final_score']:.1f})\n"
        
        report += f"""
## RecomendaÃ§Ãµes

{chr(10).join(f"- {rec}" for rec in summary['recommendations'])}

## ConclusÃµes para o TCC

1. **ValidaÃ§Ã£o do Conceito**: O sistema multiagente com ciclo crÃ­tico demonstra melhoria mensurÃ¡vel de qualidade
2. **EficÃ¡cia das IteraÃ§Ãµes**: IteraÃ§Ãµes adicionais produzem melhorias atÃ© um ponto Ã³timo
3. **Aplicabilidade PrÃ¡tica**: Sistema viÃ¡vel para produÃ§Ã£o de conteÃºdo automatizado de qualidade

---
*RelatÃ³rio gerado automaticamente em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report


def main():
    """FunÃ§Ã£o principal - interface de linha de comando"""
    
    print("=" * 60)
    print("ğŸ¤– AutonoWrite - Sistema Multiagente para GeraÃ§Ã£o de ConteÃºdo")
    print("=" * 60)
    
    # ConfiguraÃ§Ã£o do LLM
    print("\nğŸ”§ Configurando sistema...")
    try:
        llm = LLMProvider("auto")
        system = AutonoWriteSystem(llm)
        print("âœ… Sistema configurado com sucesso!")
    except Exception as e:
        print(f"âŒ Erro na configuraÃ§Ã£o: {e}")
        print("ğŸ’¡ Dica: Configure GROQ_API_KEY no arquivo .env ou use simulaÃ§Ã£o")
        return
    
    # Menu principal
    while True:
        print("\n" + "=" * 40)
        print("MENU PRINCIPAL")
        print("=" * 40)
        print("1. ğŸ“ Gerar conteÃºdo Ãºnico")
        print("2. ğŸ§ª Executar experimento comparativo")
        print("3. ğŸ“Š Ver histÃ³rico de execuÃ§Ãµes") 
        print("4. âš™ï¸ ConfiguraÃ§Ãµes")
        print("5. ğŸšª Sair")
        
        choice = input("\nEscolha uma opÃ§Ã£o (1-5): ").strip()
        
        if choice == "1":
            generate_single_content(system)
        elif choice == "2":
            run_experiment_menu(system)
        elif choice == "3":
            show_history(system)
        elif choice == "4":
            show_config_menu(system)
        elif choice == "5":
            print("\nğŸ‘‹ Obrigado por usar o AutonoWrite!")
            break
        else:
            print("âŒ OpÃ§Ã£o invÃ¡lida. Tente novamente.")


def generate_single_content(system: AutonoWriteSystem):
    """Gera conteÃºdo Ãºnico"""
    print("\nğŸ“ GERAÃ‡ÃƒO DE CONTEÃšDO ÃšNICO")
    print("-" * 30)
    
    # Input do tÃ³pico
    topic = input("Digite o tÃ³pico: ").strip()
    if not topic:
        print("âŒ TÃ³pico nÃ£o pode estar vazio")
        return
    
    # ConfiguraÃ§Ã£o de iteraÃ§Ãµes
    max_iter = input(f"MÃ¡ximo de iteraÃ§Ãµes (atual: {system.max_iterations}): ").strip()
    if max_iter.isdigit():
        max_iter = int(max_iter)
    else:
        max_iter = system.max_iterations
    
    # ExecuÃ§Ã£o
    try:
        result = system.generate_content(topic, max_iter)
        
        # Resultados
        print("\n" + "=" * 50)
        print("ğŸ“Š RESULTADOS")
        print("=" * 50)
        print(f"TÃ³pico: {result['topic']}")
        print(f"Score Final: {result['final_score']:.1f}/10")
        print(f"IteraÃ§Ãµes: {result['iterations_used']}/{result['max_iterations']}")
        print(f"Status: {'âœ… APROVADO' if result['approved'] else 'âš ï¸ NÃƒO APROVADO'}")
        print(f"Tempo: {result['execution_time_seconds']:.1f}s")
        print(f"Chamadas LLM: {result['llm_calls']}")
        
        # Salvar resultado
        save = input("\nSalvar resultado? (s/N): ").strip().lower()
        if save in ['s', 'sim', 'y', 'yes']:
            filepath = system.save_result(result)
            
            # Mostrar conteÃºdo
            show_content = input("Mostrar conteÃºdo gerado? (s/N): ").strip().lower()
            if show_content in ['s', 'sim', 'y', 'yes']:
                print("\n" + "=" * 50)
                print("ğŸ“„ CONTEÃšDO GERADO")
                print("=" * 50)
                print(result['final_content'])
                print("=" * 50)
    
    except Exception as e:
        print(f"âŒ Erro durante execuÃ§Ã£o: {e}")


def run_experiment_menu(system: AutonoWriteSystem):
    """Menu de experimentos"""
    print("\nğŸ§ª EXPERIMENTOS COMPARATIVOS")
    print("-" * 30)
    
    # TÃ³picos predefinidos para TCC
    default_topics = [
        "Sistemas Multiagente em InteligÃªncia Artificial",
        "Engenharia de Prompt para Modelos de Linguagem",
        "Ciclos de CrÃ­tica e Refinamento em IA Generativa",
        "Framework CrewAI para AutomaÃ§Ã£o de Tarefas",
        "AvaliaÃ§Ã£o de Qualidade em GeraÃ§Ã£o AutomÃ¡tica de Texto"
    ]
    
    print("OpÃ§Ãµes:")
    print("1. Usar tÃ³picos padrÃ£o (recomendado para TCC)")
    print("2. Definir tÃ³picos customizados")
    
    choice = input("Escolha (1-2): ").strip()
    
    if choice == "1":
        topics = default_topics
        print(f"âœ… Usando {len(topics)} tÃ³picos padrÃ£o")
    elif choice == "2":
        topics = []
        print("Digite os tÃ³picos (Enter vazio para finalizar):")
        while True:
            topic = input(f"TÃ³pico {len(topics)+1}: ").strip()
            if not topic:
                break
            topics.append(topic)
        
        if not topics:
            print("âŒ Nenhum tÃ³pico definido")
            return
    else:
        print("âŒ OpÃ§Ã£o invÃ¡lida")
        return
    
    # ConfiguraÃ§Ãµes do experimento
    iterations_input = input("ConfiguraÃ§Ãµes de iteraÃ§Ãµes (ex: 1,2,3): ").strip()
    if iterations_input:
        try:
            iterations_list = [int(x.strip()) for x in iterations_input.split(',')]
        except:
            iterations_list = [1, 2, 3]
    else:
        iterations_list = [1, 2, 3]
    
    print(f"ğŸ”§ ConfiguraÃ§Ãµes:")
    print(f"   - TÃ³picos: {len(topics)}")
    print(f"   - IteraÃ§Ãµes: {iterations_list}")
    print(f"   - Total de testes: {len(topics) * len(iterations_list)}")
    
    confirm = input("\nExecutar experimento? (s/N): ").strip().lower()
    if confirm not in ['s', 'sim', 'y', 'yes']:
        return
    
    # ExecuÃ§Ã£o do experimento
    try:
        runner = ExperimentRunner(system)
        result = runner.run_comparative_experiment(topics, iterations_list)
        
        # Gerar e salvar relatÃ³rio
        report = runner.generate_tcc_report(result)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"relatorio_tcc_{timestamp}.md"
        
        os.makedirs("reports", exist_ok=True)
        report_path = os.path.join("reports", report_file)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“Š Experimento concluÃ­do!")
        print(f"ğŸ“„ RelatÃ³rio salvo: {report_path}")
        
        # Mostrar sumÃ¡rio
        summary = result["summary"]
        print(f"\nğŸ¯ RESUMO EXECUTIVO:")
        print(f"   - Melhor qualidade: {summary['best_quality_config']['iterations']} iter (Score: {summary['best_quality_config']['avg_score']:.1f})")
        print(f"   - Mais rÃ¡pido: {summary['fastest_config']['iterations']} iter ({summary['fastest_config']['avg_time']:.1f}s)")
        print(f"   - Melhoria total: {summary['quality_improvement_percent']}%")
        
    except Exception as e:
        print(f"âŒ Erro no experimento: {e}")


def show_history(system: AutonoWriteSystem):
    """Mostra histÃ³rico de execuÃ§Ãµes"""
    if not system.execution_log:
        print("\nğŸ“‹ Nenhuma execuÃ§Ã£o no histÃ³rico atual")
        return
    
    print(f"\nğŸ“‹ HISTÃ“RICO ({len(system.execution_log)} execuÃ§Ãµes)")
    print("-" * 50)
    
    for i, result in enumerate(system.execution_log, 1):
        print(f"{i}. {result['topic'][:40]}...")
        print(f"   Score: {result['final_score']:.1f} | Iter: {result['iterations_used']} | Tempo: {result['execution_time_seconds']:.1f}s")
        print()


def show_config_menu(system: AutonoWriteSystem):
    """Menu de configuraÃ§Ãµes"""
    print(f"\nâš™ï¸ CONFIGURAÃ‡Ã•ES ATUAIS")
    print("-" * 30)
    print(f"Provider: {system.llm.provider_type}")
    print(f"Modelo: {system.llm.model_name}")
    print(f"Max IteraÃ§Ãµes: {system.max_iterations}")
    print(f"Score MÃ­nimo: {system.min_quality_score}")
    print(f"Chamadas LLM: {system.llm.call_count}")
    
    print(f"\nOpÃ§Ãµes:")
    print(f"1. Alterar mÃ¡ximo de iteraÃ§Ãµes")
    print(f"2. Alterar score mÃ­nimo")
    print(f"3. Resetar contador de chamadas")
    print(f"4. Voltar")
    
    choice = input("Escolha (1-4): ").strip()
    
    if choice == "1":
        new_max = input(f"Novo mÃ¡ximo de iteraÃ§Ãµes (atual: {system.max_iterations}): ").strip()
        if new_max.isdigit() and int(new_max) > 0:
            system.max_iterations = int(new_max)
            print(f"âœ… MÃ¡ximo de iteraÃ§Ãµes alterado para {system.max_iterations}")
        else:
            print("âŒ Valor invÃ¡lido")
    
    elif choice == "2":
        new_score = input(f"Novo score mÃ­nimo (atual: {system.min_quality_score}): ").strip()
        try:
            score = float(new_score)
            if 0 <= score <= 10:
                system.min_quality_score = score
                print(f"âœ… Score mÃ­nimo alterado para {system.min_quality_score}")
            else:
                print("âŒ Score deve estar entre 0 e 10")
        except:
            print("âŒ Valor invÃ¡lido")
    
    elif choice == "3":
        system.llm.call_count = 0
        print("âœ… Contador resetado")


if __name__ == "__main__":
    main()